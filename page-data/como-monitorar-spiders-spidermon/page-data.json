{"componentChunkName":"component---node-modules-gatsby-theme-blog-core-src-templates-post-query-js","path":"/como-monitorar-spiders-spidermon/","result":{"data":{"site":{"siteMetadata":{"title":"Anderson Berg","social":[{"name":"twitter","url":"https://twitter.com/berg_pe"},{"name":"github","url":"https://github.com/andersonberg"}]}},"blogPost":{"__typename":"MdxBlogPost","id":"42c97f81-485b-5dbb-b2cf-26d46b34cf54","excerpt":"Uma das etapas mais importantes do web-scraping (ou raspagem de dados) é garantir a qualidade das informações extraídas. Devido ao grande…","body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Como monitorar web-crawlers com Spidermon\",\n  \"date\": \"2020-09-06 00:00\",\n  \"tags\": [\"python\", \"web-crawlers\", \"web-spiders\", \"spidermon\", \"spider\"],\n  \"category\": \"python\",\n  \"slug\": \"como-monitorar-spiders-spidermon\",\n  \"meta_description\": \"Aprendendo a utilizar o Spidermon para monitorar web-crawlers\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Uma das etapas mais importantes do web-scraping (ou raspagem de dados) \\xE9 garantir a qualidade das informa\\xE7\\xF5es extra\\xEDdas. Devido ao grande volume de dados que podem ser coletados por um spider, \\xE9 bem complicado analisar tudo manualmente. Por\\xE9m existem ferramentas para auxiliar nesse processo que v\\xE3o diminuir muito o trabalho de quem faz esse tipo de an\\xE1lise. Uma dessas ferramentas \\xE9 o Spidermon, que vou apresentar neste artigo.\"), mdx(\"p\", null, \"Spidermon \\xE9 uma extens\\xE3o do Scrapy usado para monitorar spiders. \\xC9 um framework bem robusto e, pra mim, indispens\\xE1vel nos projetos que trabalho. Com ele \\xE9 poss\\xEDvel validar a execu\\xE7\\xE3o ou os resultados produzidos por spiders. Al\\xE9m disso, voc\\xEA pode criar relat\\xF3rios e enviar notifica\\xE7\\xF5es via slack ou e-mail, por exemplo.\"), mdx(\"p\", null, \"Para poder ver o Spidermon em a\\xE7\\xE3o, vamos come\\xE7ar criando um spider bem simples. Vamos supor que queremos extrair t\\xEDtulo do livro e respectivo ranking da seguinte p\\xE1gina: \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books/\"\n  }), \"https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books/\")), mdx(\"p\", null, \"Nosso spider seria assim:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"class BooksSpider(scrapy.Spider):\\n    name = 'books'\\n    allowed_domains = ['goodreads.com']\\n    start_urls = [\\n        'https://www.goodreads.com/list/show/3.Best_Science_Fiction_Fantasy_Books/'\\n    ]\\n\\n    def parse(self, response):\\n        books_list = response.css(\\n            'tr[itemtype=\\\"http://schema.org/Book\\\"]')\\n        for book in books_list:\\n            yield{\\n                'title': book.css(\\n                    '.bookTitle span[itemprop=\\\"name\\\"]::text'\\n                    ).get(),\\n                'rating': float(\\n                    book.css('.minirating::text'\\n                    ).re_first(r'\\\\d[\\\\d.]*'))\\n            }\\n\\n        yield scrapy.Request(\\n            response.urljoin(\\n                response.css('.next_page::attr(href)').get()\\n            )\\n        )\\n\")), mdx(\"p\", null, \"Antes de criar o primeiro monitor, habilite o Spidermon nas configura\\xE7\\xF5es do projeto. \\xC9 bem simples, adicione as seguintes linhas no arquivo settings.py do seu projeto scrapy:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"SPIDERMON_ENABLED = True\\nEXTENSIONS = {\\n    'spidermon.contrib.scrapy.extensions.Spidermon': 500,\\n}\\n\")), mdx(\"h2\", {\n    \"id\": \"criando-monitores\"\n  }, \"Criando monitores\"), mdx(\"p\", null, \"Agora para que o Spidermon comece a monitorar os resultados, precisamos criar monitores. Monitores no Spidermon s\\xE3o baseados no unittest, portanto s\\xE3o basicamente casos de teste executados em determinado momento da execu\\xE7\\xE3o do spider. Os monitores por sua vez, podem ser agrupados em um MonitorSuite. As classes MonitorSuite s\\xE3o importantes para definir quais monitores ser\\xE3o executados e quais a\\xE7\\xF5es ter\\xE3o lugar antes e depois que o grupo de monitores terminar seu trabalho.\"), mdx(\"p\", null, \"Vamos entender melhor com um exemplo: Queremos garantir que o spider vai extrair pelo menos 10 itens. Atrav\\xE9s dos stats finais do spider, temos acesso a um atributo chamado \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"item_scraped_count\"), \", que mostra quantos itens o spider conseguiu extrair. E a\\xED voc\\xEA pode usar o \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"assertTrue\"), \" para comparar os valores:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"from spidermon import Monitor, MonitorSuite, monitors\\n\\n@monitors.name('Item count')\\nclass ItemCountMonitor(Monitor):\\n\\n    @monitors.name('Minimum number of items')\\n    def test_minimum_number_of_items(self):\\n        item_extracted = getattr(\\n            self.data.stats, 'item_scraped_count', 0)\\n        minimum_threshold = 10\\n\\n        msg = 'Extracted less than {} items'.format(\\n            minimum_threshold)\\n        self.assertTrue(\\n            item_extracted >= minimum_threshold, msg=msg\\n        )\\n\\nclass SpiderCloseMonitorSuite(MonitorSuite):\\n\\n    monitors = [\\n        ItemCountMonitor,\\n    ]\\n\")), mdx(\"p\", null, \"Crie um arquivo chamado monitors.py e salve esse c\\xF3digo.\"), mdx(\"p\", null, \"A ideia \\xE9 que este MonitorSuite execute ap\\xF3s o spider terminar a execu\\xE7\\xE3o, ent\\xE3o o pr\\xF3ximo passo \\xE9 incluir mais uma configura\\xE7\\xE3o no settings.py:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"SPIDERMON_SPIDER_CLOSE_MONITORS = (\\n    'tutorial.monitors.SpiderCloseMonitorSuite',\\n)\\n\")), mdx(\"p\", null, \"Agora \\xE9 s\\xF3 executar o spider normalmente para visualizar os monitores no log.\"), mdx(\"h2\", {\n    \"id\": \"validando-itens\"\n  }, \"Validando itens\"), mdx(\"p\", null, \"Uma das principais formas de garantir a qualidade dos dados extra\\xEDdos \\xE9 valid\\xE1-los segundo um modelo. Este modelo pode ser definido em um (JSON schema)\", \"[https://json-schema.org/]\", \". Entre outras coisas \\xE9 poss\\xEDvel definir os campos obrigat\\xF3rios dos itens ou usar uma express\\xE3o regular para validar o conte\\xFAdo de um campo.\"), mdx(\"p\", null, \"Antes de utilizar a valida\\xE7\\xE3o com o JSON schema, \\xE9 preciso instalar o pacote \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"jsonschema\"), \", se j\\xE1 n\\xE3o estiver instalado:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-bash\"\n  }), \"    $ pip install jsonschema\\n\")), mdx(\"p\", null, \"Crie um arquivo chamado schema.json na raiz do projeto ou num diret\\xF3rio espec\\xEDfico para esses arquivos (s\\xF3 a n\\xEDvel de organiza\\xE7\\xE3o). Nele insira o seguinte c\\xF3digo:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-yaml\"\n  }), \"{\\n    \\\"$schema\\\": \\\"http://json-schema.org/draft-07/schema#\\\",\\n    \\\"type\\\": \\\"object\\\",\\n    \\\"properties\\\": {\\n        \\\"title\\\":{\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"rating\\\":{\\n            \\\"type\\\": \\\"number\\\"\\n        }\\n    },\\n    \\\"required\\\":[\\n        \\\"title\\\",\\n        \\\"rating\\\"\\n    ]\\n}\\n\")), mdx(\"p\", null, \"Mais algumas configura\\xE7\\xF5es s\\xE3o necess\\xE1rias serem adicionadas ao settings.py. Primeiro, habilite o \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"item pipeline\"), \" e depois adicione o caminho do arquivo criado anteriormente no \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SPIDERMON_VALIDATION_SCHEMAS\"), \":\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"ITEM_PIPELINES = {\\n    'spidermon.contrib.scrapy.pipelines.ItemValidationPipeline': 800,\\n}\\nSPIDERMON_VALIDATION_SCHEMAS = [\\n    '/home/anderson/code/crawler_test_spidermon/crawler_test_spidermon/schema.json'\\n]\\n\")), mdx(\"p\", null, \"O Spidermon vai exibir os erros de \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"schema\"), \" nos \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"stats\"), \" ao final da execu\\xE7\\xE3o do spider. Se preferir, voc\\xEA pode adicionar os erros de valida\\xE7\\xE3o nos pr\\xF3prios itens, isto pode facilitar na hora de encontrar o item que est\\xE1 fora do padr\\xE3o definido. Para isso adicione o seguinte no settings.py:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"SPIDERMON_VALIDATION_ADD_ERRORS_TO_ITEMS = True\\n\")), mdx(\"h2\", {\n    \"id\": \"enviando-notificações-via-telegram\"\n  }, \"Enviando notifica\\xE7\\xF5es via Telegram\"), mdx(\"p\", null, \"Esta \\xE9 outra fun\\xE7\\xE3o bem interessante do Spidermon. Voc\\xEA n\\xE3o precisa ficar checando um spider toda vez que ele finaliza a execu\\xE7\\xE3o para poder ver se houve algum erro. O Spidermon pode enviar notifica\\xE7\\xF5es via Email, Slack, Telegram entre outros. Neste artigo vou cobrir o envio de mensagens pelo Telegram, mas a documenta\\xE7\\xE3o do Spidermon \\xE9 bem completa com rela\\xE7\\xE3o aos outros.\"), mdx(\"p\", null, \"Para usar essa funcionalidade, voc\\xEA precisa de um (token de bot)\", \"[https://core.telegram.org/bots]\", \" do Telegram e adicionar as seguintes op\\xE7\\xF5es no settings.py:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"SPIDERMON_TELEGRAM_SENDER_TOKEN = '<TELEGRAM_SENDER_TOKEN>'\\nSPIDERMON_TELEGRAM_RECIPIENTS = ['chatid', '@channelname']\\n\")), mdx(\"p\", null, \"A op\\xE7\\xE3o \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"SPIDERMON_TELEGRAM_RECIPIENTS\"), \" cont\\xE9m todos os destinat\\xE1rios que ir\\xE3o receber as notifica\\xE7\\xF5es do Spidermon.\"), mdx(\"p\", null, \"Em seguida, no arquivo monitors.py, importe e adicione a classe que executa a a\\xE7\\xE3o de envio de mensagem ao Telegram:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {\n    \"className\": \"language-python\"\n  }), \"from spidermon.contrib.actions.telegram.notifiers import SendTelegramMessageSpiderFinished\\n...\\n\\nclass SpiderCloseMonitorSuite(MonitorSuite):\\n\\n    monitors = [\\n        ItemCountMonitor,\\n    ]\\n    monitors_failed_actions = [SendTelegramMessageSpiderFinished]\\n\")), mdx(\"p\", null, \"Pronto, se houver algum erro definido nos monitores, a notifica\\xE7\\xE3o ser\\xE1 enviada via Telegram para todos os destinat\\xE1rios.\"));\n}\n;\nMDXContent.isMDXComponent = true;","slug":"/como-monitorar-spiders-spidermon/","title":"Como monitorar web-crawlers com Spidermon","tags":["python","web-crawlers","web-spiders","spidermon","spider"],"date":"September 06, 2020","image":null,"imageAlt":null,"imageCaptionText":null,"imageCaptionLink":null,"socialImage":null},"previous":{"__typename":"MdxBlogPost","id":"31997af4-9c2f-5eeb-829a-f81ef679450f","excerpt":"Há uns dois anos atrás, eu fiz um curso no Coursera chamado  Aprendendo a Aprender  (em inglês \"Learning how to learn\"). É um curso de…","slug":"/aprendendo-a-aprender/","title":"Aprendendo a Aprender","date":"June 05, 2020"},"next":{"__typename":"MdxBlogPost","id":"a0adba68-fc4f-5a85-92b3-0fce0110b18a","excerpt":"Diversas vezes quando você vai fazer análise de um conjunto grande de dados é preciso agregar esses dados para que eles façam mais sentido e…","slug":"/pandas-groupby/","title":"Pandas Groupby","date":"February 01, 2021"}},"pageContext":{"id":"42c97f81-485b-5dbb-b2cf-26d46b34cf54","previousId":"31997af4-9c2f-5eeb-829a-f81ef679450f","nextId":"a0adba68-fc4f-5a85-92b3-0fce0110b18a","maxWidth":1380}},"staticQueryHashes":["2744905544","3090755652","386998304","764694655"]}